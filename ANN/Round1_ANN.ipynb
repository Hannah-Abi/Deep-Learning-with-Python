{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d6c976079e2daffb5f0f0d629c39864a",
     "grade": false,
     "grade_id": "cell-acf73c5193fea2fb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<h1><center>Artificial Neural Networks</center></h1>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<center><font size=\"3\">This notebook is a part of teaching material for CS-EJ3311 - Deep Learning with Python</font></center>\n",
    "<center><font size=\"3\">24.10.-11.12.2022</font></center>\n",
    "<center><font size=\"3\">Aalto University & FiTech.io</font></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "60ab252949eeca2242ad55ae2a42a4e2",
     "grade": false,
     "grade_id": "cell-efd0311f1ad9124c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Being a subset of Machine learning (ML) methods, deep learning follows the basic ML principle: find a hypothesis map out of a hypothesis space that minimizes prediction error on any data point. This notebook teaches you how to use an **artificial neural network (ANN)** to represent a hypothesis space that includes highly non-linear functions. By varying the parameters of an ANN we can select different hypothesis maps from the hypothesis space. We train an ANN by tuning its parameters such that resulting hypothesis incurs a minimum average loss on a given set of data points (the training data). The recent popularity of deep learning might be attributed partly to the availability of easy-to-use libraries for the design and training ANN. One such library is provided in the Python package `Keras`(https://keras.io/) and will be the main toolkit for this course. This notebook will show how to use  Keras to train an ANN for a simple classification task. The task is to predict the category for a shop item based on a low-resolution image of the item.\n",
    "\n",
    "**Learning Goals.** After completing this round, you should \n",
    "\n",
    "- understand that ANNs represent non-linear predictor maps that take **features as input and output a predicted label**\n",
    "- know that ANNs have adjustable/trainable/tunable parameters referred to as **weights** and **biases** \n",
    "- understand the functionality of neurons as the main **building blocks of an ANN** \n",
    "- understand how ANN parameters are **adjusted (learnt) by minimizing a loss function** using optimization methods (e.g., gradient descent)\n",
    "- be able **to construct** an ANN using `Keras` \n",
    "- be able to **train** the ANN parameters using training data\n",
    "- be able to **critically evaluate** the quality of the trained ANN \n",
    "- be able **to save and load** ANN parameters and whole model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7d67d77250303256adae60053c6eed99",
     "grade": false,
     "grade_id": "cell-7ced525e7396da95",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Additional Reading\n",
    "\n",
    "- Chapter 3 of \"Deep Learning with Python\" by F. Chollet. \n",
    "\n",
    "- Chapter 10 of \"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\" by Aurélien Géron.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "58b28ced955fa88792345afd733cccec",
     "grade": false,
     "grade_id": "cell-7babe8664923aa27",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## The Big Picture\n",
    "\n",
    "Remember the goal of ML: Find a hypothesis that allows to predict the label of a data point based on its features. Mathematically, a hypothesis is nothing but a map $h$ that reads in the features $\\mathbf{x}$ of a data point and outputs a prediction $h(\\mathbf{x})$ of its label. What sets deep learning methods apart from other ML methods is their common approach for representing a hypothesis map. Deep learning use a signal-flow chart representation, referred to as an artificial neural network, to represent a hypothesis map $h$. This signal-flow chart consists of interconnected elementary units (neurons) that include tunable parameters, referred to as weights and biases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "11886c345d4994db71572994210b3d2b",
     "grade": false,
     "grade_id": "cell-c953b240141101e6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Artificial Neurons - The Atom of Artificial Neural Networks\n",
    "\n",
    "As its name indicates, an ANN is a network of interconnected elementary computational units. These computational units are referred to as **artificial neurons** or just **neurons**, **nodes** or **units**. The simplest ANN consists of a single artificial neuron:  \n",
    "\n",
    "<img src=\"../../../coursedata/ANN/perceptron.png\" width=\"500\">\n",
    "\n",
    "Let us assume that the neuron has three inputs $x_1,x_2,x_3$. These inputs are weighted using the weights $w_1,w_2,w_3$ and summed up in the intermediate quantity \n",
    "$$z = b+w_{1}x_{1}+w_{2}x_{2}+w_{3}x_{3} $$ \n",
    "\n",
    "Note that we also added a constant term $b$ to the weighted inputs. This term is often referred to as a **bias** or **offset**. Instead of including the bias term we could equivalently add a fourth input $x_4$ and define its value to be fixed to $x_4=1$. The corresponding weight $w_4$ would then take the role of the bias $b$ in the above equation.  \n",
    "\n",
    "The artificial neuron then applies a non-linear **activation function** $g(\\cdot)$ to the weighted sum $z$. The final output of the neuron is the function value $g(z)$, referred to as the **activation** of the neuron. The activation function $g(\\cdot)$ is a design parameter that could be optimized using any method for hyperparameter turning. However, most deep learning methods use one of few popular choices for the activation function. \n",
    "\n",
    "In general deep learning methods use non-linear activation functions $g(\\cdot)$. Indeed, if we would use a linear activation function $g(z) = a z + b$, any ANN (no matter how many neurons it contains) can only produce a linear map between its input and output.  A less trivial choice for the activation function is the **sigmoid function** $g(z) = 1/(1+e^{-z})$. Another popular choice for the activation function is the **rectified linear unit** (ReLU) $g(z) = \\mbox{max} \\{z,0\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f7149765e11b0ee4397f8d69ec47bc1f",
     "grade": false,
     "grade_id": "cell-559062beb588e8a8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np                   # provides methods for processing and manipulating numerical arrays\n",
    "import matplotlib.pyplot as plt      # provides tools for plotting data\n",
    "\n",
    "from utils import load_styles\n",
    "\n",
    "# This MUST be the last line of this cell\n",
    "load_styles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c433b38a94c4adcb5cbb3e76cf47cec6",
     "grade": false,
     "grade_id": "cell-3e87d70ea0e13550",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='1.1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task.</b> Build an Artificial Neuron.</h3> \n",
    "\n",
    "[Points: 0.5p]\n",
    "\n",
    "This task requires you to implement a single artificial neuron. \n",
    "    \n",
    "The feature vector is stored as a numpy array `x` of shape `(3,1)`, i.e., the feature $x_{1}$ is given by `x[0,0]`, the feature $x_{2}$ by `x[1,0]` and feature $x_{3}$ by `x[2,0]`. Similarly the weight vector is stored as a numpy array `w` of shape `(3,1)` and the bias as a `float` variable `b`. \n",
    "\n",
    "Your task is to complete the Python snippet by: \n",
    "\n",
    "- computing the weighted sum of input $z = b+w_{1}x_{1}+w_{2}x_{2}+w_{3}x_{3}$, given the weight vector **w** and bias $b$\n",
    "- applying sigmoid activation $g(z) = 1/(1+e^{-z})$ to a weighted sum $z$\n",
    "\n",
    "Note: $g$ should be a floating-point number, not a numpy array.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "143d5aef8837ca40b95bda54269ca7a2",
     "grade": false,
     "grade_id": "cell-b10510eb489350e1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<details>\n",
    "    <summary>\n",
    "        <span class=\"summary-title\">Hints</span>\n",
    "    </summary>\n",
    "    <div class=\"summary-content\">\n",
    "    <ul>\n",
    "       <li>\n",
    "           To compute the weighted sum you may use the dot (or inner) product between feature vector <strong>x</strong> and weight vector \n",
    "           <strong>w</strong>. You can use numpy function <code>np.dot()</code> or the operator <code>@</code> to compute the inner product \n",
    "           between two vectors stored in numpy arrays. Alternatively, you can perform the operation manually element-by-element and sum up \n",
    "           the result.\n",
    "       </li>     \n",
    "       <li>\n",
    "               You can compute the exponential function  <i>e<sup>a</sup></i>, for some number <i>a</i>, using the numpy function <code>np.exp(a)</code>\n",
    "        </li>    \n",
    "     </ul>\n",
    "     </div>    \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5de4c159d034cab1318f52012f16b812",
     "grade": false,
     "grade_id": "cell-6470a158ac0f49bf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# input/feature vector\n",
    "x = np.array([0.14, 0.2, -5]).reshape(3,1)\n",
    "\n",
    "# bias\n",
    "b = 0.5\n",
    "\n",
    "# weight vector\n",
    "w = np.array([10, 5, 0.3]).reshape(3,1)\n",
    "\n",
    "# remove the line raise NotImplementedError() before testing your solution and submitting code\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()\n",
    "# compute weighted sum\n",
    "z = b + np.dot(np.transpose(x),w)\n",
    "# apply sigmoid function to weighted sum z\n",
    "g = (1/(1 + 1/np.exp(z))).item()\n",
    "# print the results\n",
    "print(f\"The output is: {g:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea79c19af1154105b7e664969dd5c1e6",
     "grade": false,
     "grade_id": "cell-eeea177da3c4438d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Perform some sanity checks on the solution\n",
    "assert g <1, \"Value of g is incorrect!\"\n",
    "assert g >0, \"Value of g is incorrect!\"\n",
    "\n",
    "print(\"Sanity checks passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "67287013fd1273e7075fbe0f2b67bd40",
     "grade": true,
     "grade_id": "cell-07b0b308ea453454",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# this cell is for hidden test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dad1a1c5e3e1e0c5ccab079c95422f8f",
     "grade": false,
     "grade_id": "cell-3a3efb4dd9533c2b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Organizing Neurons as Layers \n",
    "\n",
    "Deep learning methods use ANNs containing billions of interconnected neurons. It turns out to be beneficial, both statistically and computationally, to not allow for completely arbitrary connections between neurons but require a particular global order or architecture. Many of the most successful deep learning methods use ANNs that consist of consecutive layers of neurons. Each layer is made of multiple individual neurons whose input is the outputs of neurons in the previous layer. We refer to ANN that consist of a sequence or chain of many layers as **deep neural networks**. \n",
    "\n",
    "There has been some recent progress in the understanding of the statistical and computational properties of deep nets. However, we are still lacking a solid theory for guiding the choice of the number of layers (the depth of ANN) and the size of layers (width) for a given dataset. In practice, the application of deep learning methods still requires a lot of manual experimentation (\"trial and error\"). A typical approach to applying deep learning is to start from an existing ANN structure that has been proven useful for related datasets and then modify (tailor) it to the application at hand (we will discuss this in the notebook Transfer Learning). \n",
    "\n",
    "The figure below illustrates an ANN with four layers: input layer, two hidden layers, and output layer. The hidden layers are called \"hidden\" because in contrast to input and output layers we cannot directly access the inputs and outputs of the hidden layers. The neurons of the hidden layers are called hidden neurons.\n",
    "\n",
    "The index in the braces indicates to which layer the parameters belong to.\n",
    "\n",
    "**Input layer**. A convention of the deep learning literature is to refer to the input  $x_{1},\\ldots,x_{n}$ of an ANN as the input layer. This input layer consists solely of the individual features of a data point. The input layer sends these features directly to the first hidden layer. The example below includes 3 features values $x_{1},x_{2},x_{3}$.\n",
    "\n",
    "**First hidden layer**. This layer consists of five neurons. Inputs for a neuron in this layer are the outputs of each neuron in the previous layer. These type of layers are therefore called **fully connected** or **dense layers**. For each hidden neuron we compute linear combination of vectors $\\mathbf{w}$ and $\\mathbf{x}$ (i.e. weighted sum of feature values) and apply non-linearity to the obtained result.\n",
    "\n",
    "**Second hidden layer**. Similar to first hidden layer, this layer is fully connected to its preceding layer.  Inputs for a neuron in this layer are the outputs of the hidden neurons in the previous layer. Again, we compute linear combination of a weight vector (weight values between 1st and 2nd hidden layers) and input vector (output of the first hidden layer) and apply non-linearity to the obtained result.\n",
    "\n",
    "**Output layer**. In this particular example, the final layer consists of just one output neuron. The output value may represent the confidence (probability) in assigning the data point to a positive class (binary classification). In the case of regression problem, output neuron returns only the weighted sum (activation function is not used).\n",
    "\n",
    "\n",
    "<img src=\"../../../coursedata/ANN/ANN.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bd6755817b806754f19daa4cc76e86ac",
     "grade": false,
     "grade_id": "cell-bcdc1a2add806b8a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## A Closer Look At Activation Functions\n",
    "\n",
    "As we discussed before, a neuron applies a non-linear activation function to weighted sum of its inputs. You will rarely encounter an ANN using linear activation functions or even omitting the activation function and directly using the weighted sum of the input as the neuron output. Indeed, an ANN using only linear activation functions can only implement a linear map from the input features to the output. No matter how many layers we stack on top of each other, the overall behavior of the ANN will always be a linear map. \\\n",
    "For example, let's take a simple ANN with two neurons in a hidden layer and one output neuron without adding any activation function. For a data point with only one feature $x$ the hidden neurons will output weighted sums $z_{1} = b^{(1)}_{1}+w^{(1)}_{1}x$ and $z_{2} = b^{(1)}_{2}+w^{(1)}_{2}x$, where $b^{(1)}_{1}$ and $w^{(1)}_{1}$ are the bias and weight for a first hidden neuron and $b^{(1)}_{2}$ and $w^{(1)}_{2}$ are the bias and weight for a second hidden neuron. The index in the braces indicates to which layer the parameters belong to.\\\n",
    "The figure below shows the outputs of these hidden neurons (blue line) for a range of values $x \\in \\{-10,10\\}$. The output of the last neuron can be described as a function $h(x) = b^{(2)}_{1}+w^{(2)}_{1}z_{1}+w^{(2)}_{2}z_{2}$, which is just a linear combination of $z_{1}$ and $z_{2}$. In other words, it doesn't matter how we will change the bias and weight values of the neurons - the network will always return a linear predictor.\n",
    "<img src=\"../../../coursedata/ANN/lincomb.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fd4d7d970f1ab1e1fdbb847e80df5962",
     "grade": false,
     "grade_id": "cell-4cca1c0f6e278e54",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='2.2'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student task.</b> Combining Linear Functions gives a Linear Function.</h3> \n",
    "\n",
    "Points: 0.25p\n",
    "    \n",
    "This task requires you to implement a hypothesis map $h(x)$ using the simple ANN depicted in the figure above. The weights and bias terms of the neurons in the hidden layer are, respectively, fixed to $b^{(1)}_{1}=-2, \\;w^{(1)}_{1}=-1,b^{(1)}_{2}=-2, w^{(1)}_{2}=1$ . For the weights and bias terms of the output neuron, use $b^{(2)}_{1}=0.5,  \\;w^{(2)}_{1}=5,  \\;w^{(2)}_{2}=3$. Apply the resulting map $h(x)$ to the feature values stored in the numpy array `x` which is already defined in the code snippet below. \n",
    "\n",
    "**Note!**\n",
    "\n",
    "- Here a datapoint has only one feature and feature values of all (100) datapoints are stacked in a numpy array `x` of shape `(100,1)`. The entry `x[0,0]` represents the feature of the first data point, the entry `x[1,0]` is the feature of the second data point and so on. \n",
    "\n",
    "\n",
    "- The resulting numpy array `h` should also be of shape `(100,1)`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a9c774c76372d6dc94730b7d0a87bdca",
     "grade": false,
     "grade_id": "cell-d4b16b7112e47aee",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# feature values of 100 data points stacked in a vector `x`\n",
    "x = np.linspace(-10, 10, 100).reshape(100,1)\n",
    "\n",
    "# weight and bias of first hidden neuron\n",
    "w11, b11 = -1, -2\n",
    "# weight and bias of second hidden neuron\n",
    "w12, b12 = 1, -2\n",
    "\n",
    "# weights and bias of output neuron\n",
    "b21 = 0.5\n",
    "w21, w22 = 5, 3\n",
    "\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()\n",
    "\n",
    "# compute weighted sum for two hidden neurons\n",
    "z1 = b11 + x*w11\n",
    "z2 = b12 + x*w12\n",
    "# compute weighted sum of hidden neurons' outputs (without activation)\n",
    "h = b21 + z1*w21 + z2*w22\n",
    "# plot outputs of neurons\n",
    "fig, axes = plt.subplots(1,3, sharey=True, figsize=(8,2))\n",
    "\n",
    "# output of first hidden neuron\n",
    "axes[0].plot(x, z1)\n",
    "# output of second hidden neuron\n",
    "axes[1].plot(x, z2)\n",
    "# output of output neuron\n",
    "axes[2].plot(x, h)\n",
    "\n",
    "axes[0].set_title('$z_{1} = b^{(1)}_{1} + w^{(1)}_{1}x$', fontsize=12)\n",
    "axes[1].set_title('$z_{1} = b^{(1)}_{2} + w^{(1)}_{2}x$', fontsize=12)\n",
    "axes[2].set_title('$h(x) = b^{(2)}_{1} + w^{(2)}_{1}z_{1} + w^{(2)}_{2}z_{2}$', fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e8852d2acdce0ce954de6f016ab465b6",
     "grade": false,
     "grade_id": "cell-70a390afebaf365e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Perform sanity checks on the solution\n",
    "assert h.shape == (100,1), \"Shape h should be (100,1)\"\n",
    "assert (h[42]<-10)&(h[0]<5), \"Values of h are incorrect!\"\n",
    "\n",
    "print(\"Sanity checks passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "814e6597c27dd23a4d91e4d1300af1d1",
     "grade": true,
     "grade_id": "cell-7c29a8981294498d",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# this cell is for hidden test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cc8cb8579fe500a89c8dc7d91dda346f",
     "grade": false,
     "grade_id": "cell-1a40204a60406e42",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "Thus, using ANNs with linear activation functions is equivalent to using linear predictor maps. No matter how many layers we add, as long as we only use linear activation functions, we end up with a linear map from the features of a data point to its predicted label. However, in many relevant applications, the relation between features (e.g., colour intensities of image pixels) and label (e.g., presence of a cat in the image) is highly non-linear.\n",
    "\n",
    "To learn non-linear predictor maps, we need to use ANNs with non-linear activation functions. There are no widely applicable guidelines that suggest which particular activation function to use for a given application. However, some choices have been proven useful in many application domains. Two such choices for the activation function are the \n",
    "\n",
    "\\begin{equation} \n",
    "\\mbox{rectified linear unit (ReLU) } g(z) = {\\rm max} \\{0,z\\}, \n",
    "\\end{equation}\n",
    "\n",
    "and the \n",
    "\n",
    "\\begin{equation} \n",
    "\\mbox{sigmoid function } g(z) = 1/(1+{e}^{-z}). \n",
    "\\end{equation} \n",
    "\n",
    "\\\n",
    "<img src=\"../../../coursedata/ANN/activation.png\" width=\"600\">\n",
    "\n",
    "\n",
    "You can find plots of other popular choices for the activation function [here](https://cdn-images-1.medium.com/max/1000/1*4ZEDRpFuCIpUjNgjDdT2Lg.png). \n",
    "\n",
    "Learning the weights for a single neuron with a sigmoid activation function, and using the cross-entropy loss, is equivalent to logistic regression. You can find more details about cross-entropy loss [here](https://machinelearningmastery.com/cross-entropy-for-machine-learning/)). Background on logistic regression is provided in Chapter 3 of [http://mlbook.cs.aalto.fi](https://github.com/alexjungaalto/MachineLearningTheBasics/blob/master/MLBasicsBook.pdf). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5a413aa80e98ff1f585d74f575fc65d7",
     "grade": false,
     "grade_id": "cell-07e696c59c564798",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The figure below indicates how using neurons with ReLU activation function results in a non-linear map between the input and the output of ANN. ReLU activation function is applied to the output of the two hidden neurons $z_{1} = b_{1}^{(1)}+w_{1}^{(1)}x$ and $z_{2} = b_{2}^{(1)}+w_{2}^{(1)}x$, thus introducing nonlinearity to the network. The output neuron then can generate non-linear predictor by combining the output of the hidden neurons with activation function: \n",
    "\n",
    "$h(x) = b_{1}^{(2)}+w_{1}^{(2)}g(z_{1}) +w_{2}^{(2)}g(z_{2})$\n",
    "\n",
    "Using a sufficiently large number of hidden layers and more neurons within an ANN allows us to accurately learn highly non-linear relations between the input features and the output of an ANN. From a computational complexity point of view, the relevant parameter is not the number of neurons but the number of connections (links) between the neurons. The strength or weight of these connections must be adjusted (learned) by optimization methods. The more weights to be adjusted, the more computational resources (memory, processing time) are required during the training. The more computational resources the more expensive the training becomes.\n",
    "\n",
    "<img src=\"../../../coursedata/ANN/relucomb.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "539ab48f5617701f31b8d2fb44c8ba50",
     "grade": false,
     "grade_id": "cell-c1cef89351022571",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='2.3'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student task.</b> Combination of non-linear functions.</h3>\n",
    "\n",
    "Points: 0.25p\n",
    "\n",
    "Modify previous task to add non-linearity: apply ReLU activation function max(0,x) to outputs of hidden neurons `z1` and `z2`.     \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bfe6ffaaec98bab5152a382c6451a5d0",
     "grade": false,
     "grade_id": "cell-1dab72e8f687dcaa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<details>\n",
    "    <summary><span class=\"summary-title\">Hints</span></summary>\n",
    "    <div class=\"summary-content\">\n",
    "    To apply ReLU function max(0,x) to an array, you can:\n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>\n",
    "            create zero vector with <code>np.zeros()</code>\n",
    "         </li>\n",
    "        <li>\n",
    "            use <code>np.maximum()</code> function to do element-wise comparison between values of zero vector and <code>z1</code>,\n",
    "            <code>z2</code> arrays.\n",
    "        </li>\n",
    "    </ul>\n",
    "    </div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "40fc52b8a636f228d02a9c5ab4b877b8",
     "grade": false,
     "grade_id": "cell-514ee8d9b01994ba",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()\n",
    "\n",
    "# use weighted sum z1 and z2 computed in previous task and \n",
    "# apply ReLU activation function to z1\n",
    "g1 = np.maximum(np.zeros(z1.shape),z1)\n",
    "# apply ReLU activation function to z2\n",
    "g2 = np.maximum(np.zeros(z2.shape),z2)\n",
    "# compute weighted sum of hidden neurons' outputs \n",
    "h = b21 + g1*w21 + g2*w22\n",
    "\n",
    "# plot outputs of neurons\n",
    "fig, axes = plt.subplots(2,3, figsize=(7,4))\n",
    "\n",
    "axes[0,0].plot(x, z1) # weighted sum of first hidden neuron\n",
    "axes[0,1].plot(x, z2) # weighted sum of second hidden neuron\n",
    "axes[0,2].axis('off') # hide axis of extra subplot\n",
    "\n",
    "axes[1,0].plot(x, g1) # activation of first hidden neuron\n",
    "axes[1,1].plot(x, g2) # activation of second hidden neuron\n",
    "axes[1,2].plot(x, h) # output \n",
    "\n",
    "axes[0,0].set_title('$z_{1} = b^{(1)}_{1} + w^{(1)}_{1}x$', fontsize=12)\n",
    "axes[0,1].set_title('$z_{1} = b^{(1)}_{2} + w^{(1)}_{2}x$', fontsize=12)\n",
    "axes[1,0].set_title('$g(z_{1}) = max(0,z_{1})$', fontsize=12)\n",
    "axes[1,1].set_title('$g(z_{2}) = max(0,z_{2})$', fontsize=12)\n",
    "axes[1,2].set_title('$h(x) = b^{(2)}_{1} + w^{(2)}_{1}g(z_{1}) + w^{(2)}_{2}g(z_{2})$', fontsize=12)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "24a2352870a392166df7f45bb380d3e6",
     "grade": false,
     "grade_id": "cell-522782b506782184",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Perform some sanity checks on the solution\n",
    "assert h.shape == (100,1), \"reshape h to (100,1)\"\n",
    "assert (h[42]<1)&(h[0]<45), \"Values of h are incorrect!\"\n",
    "\n",
    "print(\"Sanity checks passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5c81bf21875e86aa459e6872e5112fbe",
     "grade": true,
     "grade_id": "cell-551d1b46ab80b54d",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# this cell is for hidden test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d9dee265a3bc03cf38ebd691dffdda75",
     "grade": false,
     "grade_id": "cell-ce672f08616925b4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The sigmoid function is a popular choice for the activation function of the neurons in the final or output layer. Indeed, the value range of the sigmoid function is the interval of real numbers between $0$ and $1$. Thus, we can interpret the output of the sigmoid activation function as a probability or degree of belonging to a certain class or category. For the neurons in the hidden layers, the ReLU activation function is a popular choice. \n",
    "\n",
    "\n",
    "<img src=\"../../../coursedata/ANN/activation_layers.png\" width=\"700\">\n",
    "\n",
    "The ANN depicted above contains an output layer with neurons with a sigmoid activation function. Each of these three output neurons could represent the degree of belonging to a certain class. The top neuron could represent the class \"Car\" (vs. \"No Car\"), the middle neuron the class \"Tree\" (vs. \"No Tree\"), and the bottom output neuron the class \"Fish\" (vs. \"No Fish\"). This representation of class labels is useful when the data point can belong to several classes at the same time (**multilabel classification**). For example, when you need to identify which objects are present on the image. There might be no cat, tree, or car present in the picture or might be only cats or two objects, car and cat, etc.\n",
    "\n",
    "**Binary classification** problems on the other hand, involve data points that might belong to only two classes. We represent the class of a data point by single label $y$. For example $y=1$ could mean \"cat\" image and $y=0$ means a \"no cat\" image. Each image is then placed into either category \"cat image\" or \"no cat image\". For binary classification, we migh use ANNs having a single output neuron with sigmoid activation. The output of sigmoid function is a real number between $0$ and $1$ and can be interpreted as probability of belonging to \"positive\" class $p(y=1)$. Probability of belonging to \"negative\" class is then $p(y=0)=1-p(y=1)$. \n",
    "\n",
    "Beside multilabel and binary classification problems, we also consider **multiclass classification** problems. Here, data points belong to exactly one out of more than two possible classes. For such multiclass classification we might use ANNs having output neurons with a **softmax** activation function. Similar to ANNs for **multilabel classification**, we also use more than one output neuron. However, in contrast to multilabel classification these output neurons are not working independently but are coupled via the **softmax** activation function\n",
    "\n",
    "\\begin{equation}\n",
    "g_{i} = \\frac{\\exp(z_{i})}{\\sum_{j} \\exp(z_{j})}. \n",
    "\\end{equation} \n",
    "Here, $g_{i}$ the output of the $i$th neuron in the output layer and the sum ranges over all neurons in that output layer. The quantity $z_{i}$ denotes the weighted input fo the $i$th output neuron.\n",
    "\n",
    "The softmax depends on all neurons belonging to the same layer. The probabilities of belonging to a certain class returned by softmax are summed up to 1, while this is not a case for multilabel classification. \n",
    "\n",
    "Let us illustrate the difference between using output neurons with independent sigmoid activation (**multilabel classification**) and using output neurons with coupled softmax activation (**multiclass classification**) by an example. We want to label images according to three categories \"car\", \"tree\" and \"fish\". We might either want to divide them into those categories (which is a **multiclass classification** problem) or we want to caption each image by objects shown in that image (which is a **multilabel classification** problem). \n",
    "\n",
    "For example, three neurons with sigmoid activation can return something like this:\n",
    "- probability of car being on the image 0.2\n",
    "- probability of tree being on the image 0.8\n",
    "- probability of fish being on the image 0.9\n",
    "\n",
    "Note, that total probability is not summing up to 1.\\\n",
    "Here we conclude, that a tree and a fish are on the picture, but not a car (given threshold of probability 0.5).\n",
    "\n",
    "Three neurons with softmax activation can return something like this:\n",
    "- probability of car being on the image 0.2\n",
    "- probability of tree being on the image 0.7\n",
    "- probability of fish being on the image 0.1\n",
    "\n",
    "Note, that total probability is summing up to 1.\\\n",
    "Here we conclude, that tree is on the picture, as it has highest probability. \n",
    "\n",
    "In conclusion, beware that the output layer and accompanying activation function defines the type of predictions that the network makes (classes are not mutually exclusive vs classes are mutually exclusive) and hence its purpose (multilabel vs multiclass classification).\n",
    "\n",
    "Here is a reference table from \"Deep Learning with Python\" F.Chollet:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "283ae51aa61baa8d95fccc8d44e978c7",
     "grade": false,
     "grade_id": "cell-595ad8ff503dcda9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img src=\"../../../coursedata/ANN/chollet.png\" id='table'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "28da127050055bece16f8ec4c4792a9d",
     "grade": false,
     "grade_id": "cell-fc912046fe7d97bc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-success\">\n",
    "    <h3><b>Try it out. </b>Activation functions.</h3>\n",
    "\n",
    "Let's demonstrate further what you've learned with an interactive neural network. Go to TensorFlow playground https://playground.tensorflow.org/ . Use the following parameters:\n",
    "\n",
    "- dataset - Gaussian\n",
    "- features ${X}_{1}$ and ${X}_{2}$\n",
    "- 2 hidden layers with 6 units each, coupled with **linear** activation function\n",
    "- learning rate 0.01\n",
    "    \n",
    "Train for about ~100 epochs. Note train and test loss values. Does ANN able to separate distinct classes? Data points are separated well if the area with the majority of orange points is colored bright orange and a zone with blue points is colored bright blue. If coloring is pale orange or pale blue, it means that ANN is not sure to which class data point belongs.\n",
    "    \n",
    "Now try out Circle dataset. What do you observe? ANN with linear activation functions does not perform very well on a linearly inseparable dataset. What about if we will add some non-linear activation (ReLU or tanh)?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "325118cbeabee8b816c68e764610be03",
     "grade": false,
     "grade_id": "cell-e2ed122c502ebe75",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## ANN Training - Finding Good Weights \n",
    "\n",
    "Consider a given ANN structure with a certain number of hidden layers and each layer consisting of many interconnected neurons. To use the ANN we also need to specify the weights for the connections between the neurons. This specification is typically done by optimization methods (referred to as [**optimizers**](https://keras.io/api/optimizers/#available-optimizers)). These methods tune the weights $\\mathbf{w}$ to minimize the average loss $f(\\mathbf{w})$ obtained by applying the ANN with weights $\\mathbf{w}$ to a training set consisting of labeled data points. These training data points are referred to as **training samples** or **training set**.\n",
    "\n",
    "It turns out that a very successful method to find good values for the weights is to use gradient-based optimization methods. These family of algorithms will be the main subject of the next notebook. For now we just mention that the idea of these methods is to iteratively update the current weight values based on (approximations of) the gradient of the training loss $f(\\mathbf{w})$. \n",
    "\n",
    "A key step of any gradient based method is the computation of gradients $\\nabla f(\\mathbf{w})$. The computation of gradients typically requires evaluating the ANN output for each data point in the training set. Training set might consist of billions of images stored in data centers all over the planet. Deep learning methods therefore do not compute exact gradients but (\"stochastic\") approximations using randomly chosen subsets (\"batches\") of the training set. Each iteration (\"gradient step\") of these methods uses only a small subset (batch) of training samples. After a certain number of iterations, which is referred to as an **epoch**, each training sample has been contained in one of these batches. \n",
    "\n",
    "To sum up ( <a href='http://faroit.com/keras-docs/2.0.2/getting-started/faq/#what-does-sample-batch-epoch-mean'>from keras docs </a>):\n",
    "\n",
    "- **Sample**: one element of a dataset.\\\n",
    "  Example: one image is a sample in a convolutional network\\\n",
    "  Example: one audio file is a sample for a speech recognition model\n",
    "  \n",
    "- **Batch**: a set of N samples. The samples in a batch are processed independently, in parallel. During training, a batch results in only one update (gradient step) to the model.\\\n",
    "A batch generally approximates the distribution of the input data better than a single input. The larger the batch, the better the approximation; however, it is also true that the batch will take longer to process and will still result in only one update. For inference (evaluate/predict), it is recommended to pick a batch size that is as large as you can afford without going out of memory (since larger batches will usually result in faster evaluating/prediction).\n",
    "\n",
    "- **Epoch**: an arbitrary cutoff, generally defined as \"one pass over the entire dataset\", used to separate training into distinct phases, which is useful for logging and periodic evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c9a672a1619c39917a632c1cee8671d5",
     "grade": false,
     "grade_id": "cell-5736a22f240b9ac4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Gradient descent (GD) algorithm is based on the following steps:\n",
    "\n",
    "**Step 1 - Initialization.** Choose a first guess (e.g. using random number generators) for the ANN weights.\n",
    "\n",
    "**Step 2 - Forward Pass.** Compute the predicted labels $\\hat{y}^{(i)}$ for a randomly chosen subset (batch) of the training set. \n",
    "\n",
    "**Step 3 - Compute loss.** Compute loss for true and predicted labels ${y}^{(i)}$ and $\\hat y^{(i)}$.\n",
    "\n",
    "**Step 4 - Backward Pass.** Compute an estimate $g(\\mathbf{w})$ for the gradient of the loss using the resulting prediction errors ${y}^{(i)} - \\hat y^{(i)}$.  \n",
    "\n",
    "**Step 5 - Gradient Step.** Update the ANN weights by taking a (small) step into the opposite direction of the gradient: $\\mathbf{w} \\mapsto \\mathbf{w} - \\alpha g(\\mathbf{w})$. \n",
    "\n",
    "**Step 6 - Stopping Criterion.** If the stopping criterion is not met, go to Step 2. \n",
    "\n",
    "\n",
    "The algorithm of GD is the same for linear regression, which uses linear predictor maps, and deep learning methods, which use non-linear predictor maps represented by an ANN. The difference between these methods is only the actual computation of the predictions (Step 2) and the gradient estimate (Step 4). \n",
    "\n",
    "Step 2 computes the output (predictions) and is referred to as the **forward pass** or forward propagation. We can  interpret the evaluation $h(\\mathbf{x})$ of the predictor map represented by the ANN as a sequence of computations whose results flow \"forward\", from the input layer to the output layer in the ANN. \n",
    "\n",
    "Step 4 computes (an estimate of) the gradient of the loss function is called **backward pass** or backward propagation (or back prop for short). This name is inspired by the temporal order of the computations used to evaluate the gradient (estimate). Loosely speaking, the gradient is computed by combining intermediate results that \"propagate\" from the output layer towards the input layer. In the end, the backward pass is nothing but a clever way to compute the gradient of a function using the \"chain rule\".\n",
    "\n",
    "We will discuss calculations of GD next week, but for now we will use ready-made Python functions provided by the Keras library. This library provides methods to specify and train ANNs. These methods require only to choose some variant of GD (called **optimizer** in Keras), to be used for tuning the ANN weights, and the corresponding GD hyperparameters. Examples of these hyperparameters are the **learning rate** (how fast ANN learns), the **mini-batch size** (size of data subset used for one gradient step), and the splitting ratio between training and validation sets. \n",
    "\n",
    "After repeating Step 2 - Step 5 of GD for a number $r$ of iterations, we obtain the weights $\\mathbf{w}^{(r)}$. To monitor the progress of the learning process we use the training loss $f\\big(\\mathbf{w}^{(r)}\\big)$ and the validation loss $f_{\\rm val}\\big(\\mathbf{w}^{(r)}\\big)$. \n",
    "\n",
    "The training loss is the average loss obtained by predicting the labels $y$ of the training data points using the predictions $\\hat{y}=h^{(\\mathbf{w}^{(r)})}(\\mathbf{x})$ with the features $\\mathbf{x}$ and the predictor map $h^{(\\mathbf{w}^{(r)})}$ represented by the ANN with weights $\\mathbf{w}^{(r)}$. Similarly, the validation loss is obtained by the prediction error incurred by $h^{(\\mathbf{w}^{(r)})}$ on the data points in the validation set (which is a different set of data points). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "530c8b5e278c868614bdc111ccb3b40e",
     "grade": false,
     "grade_id": "cell-a5f6291543d27159",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## The Python Deep Learning Library Keras\n",
    "\n",
    "Let us now introduce some basic functions provided by the Python package (library) Keras. Technically, `Keras` is a high-level API for the low-level deep learning software library [`TensorFlow`](https://www.tensorflow.org/overview/). It has been created for quick and easy model design and training. \n",
    "The typical workflow of constructing and training an ANN using Keras is as follows:\n",
    " \n",
    " 1. define the training and validation data\n",
    " 2. define the ANN structure (sequence of layers, number of neurons in each layer, activation functions)\n",
    " 3. choose a loss function, optimizer (gradient-based iterative algorithm), and some metric to monitor the learning process\n",
    " 4. tune the weights of the ANN by using the `.fit()` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "28fec03650303e34920b193243f7c8e9",
     "grade": false,
     "grade_id": "cell-aa338a31df429da6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We start with importing the Python libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "638af73b825223038adda44f576046cc",
     "grade": false,
     "grade_id": "cell-c6ecffc9d661d357",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np                  # library for numerical computations (vectors, matrices, tensors)\n",
    "import matplotlib.pyplot as plt     # library providing tools for plotting data \n",
    "import tensorflow as tf             # open source library for deep learning\n",
    "from tensorflow import keras        # library providing methods for defining and training ANN \n",
    "from tensorflow.keras import layers # layers are the basic building blocks of neural networks in Keras\n",
    "import pandas as pd                 # library for handling tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set training = False when validating or submitting notebook\n",
    "# and set training = True, when training network\n",
    "#training=True\n",
    "training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a6aba438a00ed8288bf2bf72c1c0bb4",
     "grade": true,
     "grade_id": "cell-67ab2acd5f070c77",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# this is a hidden cell to set training=False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e42cf7d3cd8776d3a8260ac981a57de1",
     "grade": false,
     "grade_id": "cell-51893ce36afef536",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 1 - Loading Data\n",
    "\n",
    "We will use the [Fashion-MNIST ](https://www.tensorflow.org/datasets/catalog/fashion_mnist) dataset which is provided within `keras.datasets`. This dataset consists of data points representing articles of an online shop. Each article is characterized by a $28 \\times 28$ pixels grayscale image. Moreover, each article is associated with a label $y$ that indicates to which of $10$ classes (or product categories) this article belongs. \n",
    "\n",
    "When you load `fashion_mnist` dataset with `fashion_mnist.load_data()`, the loading function automatically returns data split on training and test sets. The entire training dataset consists of $60000$ data points and you would need to split the training set further into the training and validation set by yourself. The test set is of size $10000$ data points. \n",
    "\n",
    "The $10000$ data points in the test set must not be used for learning the ANN weights and also not for monitoring the progress (validation) of the gradient method. Indeed, the monitoring of the validation error is some form of model adjustment as we use the validation error to decide when to stop the gradient method.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "904c461516a6b3d4a0f5cb15b73e753d",
     "grade": false,
     "grade_id": "cell-60b019828ca45aa3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# load dataset\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "(trainval_images, trainval_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "# shape of train and test image\n",
    "print(f'Number of training and validation examples {trainval_images.shape}')\n",
    "print(f'Number of test examples {test_images.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1626d6fa3a60a6532cf0c1941ada16c8",
     "grade": false,
     "grade_id": "cell-b921a7ad9b40cf5f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As you can see, the data is stored as 3D NumPy arrays, where the first dimension is the number of samples and the last two - the size of the image (28x28 pixels). Let's find out the range of feature values (pixels) and data type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "54dfb25e8fecfbd00ecaac5a255d0d88",
     "grade": false,
     "grade_id": "cell-37fc74be57659ff4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(f'Min feature value {trainval_images.min()}')\n",
    "print(f'Max feature value {trainval_images.max()}')\n",
    "print(f'Data type {type(trainval_images.min())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "29b6fa509f5a6e67288764cbdde587ce",
     "grade": false,
     "grade_id": "cell-33710746a1c19af3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The data point labels are integer values between 0 and 9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ab29d7e7f8cb3331a6aeac29006cc597",
     "grade": false,
     "grade_id": "cell-3ba7c91a26ff45f6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# the label values are stored as integer numbers, in the range [0, 9]\n",
    "# these numeric labels correspond to the classes of clothing items the image represent:\n",
    "\n",
    "labels = np.unique(test_labels)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "361b02b2e0724ef27d34b34a87d08706",
     "grade": false,
     "grade_id": "cell-14af7e11df6aec9f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The code snippet below maps the numeric label values to class names. The class names are defined according to [documentation](https://www.tensorflow.org/tutorials/keras/classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dd46fba086bc36931323859c935617e6",
     "grade": false,
     "grade_id": "cell-34de273a0e9a09d5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal',      'Shirt',   'Sneaker',  'Bag',   'Ankle boot']\n",
    "\n",
    "# display numeric label and corresponding class name \n",
    "print('label value \\t category \\n')\n",
    "for class_name, label in zip(class_names, labels):\n",
    "    print (f'{label} \\t\\t {class_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8c954a903c55ef81fb99e69abac06280",
     "grade": false,
     "grade_id": "cell-83473a5113b1c615",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's visualize a few data points (images):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aba97f4688dc257d92ec5582a6ee33b1",
     "grade": false,
     "grade_id": "cell-730cd89cd36a0cbf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# visuale 10 first images from training set\n",
    "plt.figure(figsize=(10,10))\n",
    "i = 0\n",
    "for (image, label) in zip(test_images[:10],test_labels[:10]):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([]) # remove ticks on x-axis\n",
    "    plt.yticks([]) # remove ticks on y-axis\n",
    "    plt.imshow(image, cmap='binary') # set the colormap to 'binary' \n",
    "    plt.xlabel(class_names[label])\n",
    "    i += 1\n",
    "plt.tight_layout()    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "20c0842035b80d924deb89a658a5545e",
     "grade": false,
     "grade_id": "cell-7d8ba9d5504382be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Each shop item is characterized by a feature vector $\\mathbf{x}=\\big(x_{1},\\ldots,x_{n}\\big)$ of length $n = 784$ which corresponds to $28 \\times 28$ pixels. The feature $x_{i}$ is the grayscale values of $i$th pixels. A black pixel has grayscale value 0, while a white pixel has grayscale value $255$. \n",
    "\n",
    "The code snippet below reads in one data point (shop item) and illustrates its features, i.e., grayscale values of each pixel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "faa83a6cb67d0948560686b1de1da77c",
     "grade": false,
     "grade_id": "cell-3152eeb5e58d2247",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# select the image to visualize\n",
    "img = test_images[0]\n",
    "# create figure and axis objects\n",
    "fig, ax = plt.subplots(1,1,figsize = (10,10)) \n",
    "# display image\n",
    "ax.imshow(img, cmap='gray')\n",
    "width, height = img.shape\n",
    "# this value will be needed in order to change the color of annotations\n",
    "thresh = img.max()/2.5\n",
    "\n",
    "# display grayscale value of each pixel\n",
    "for x in range(width):\n",
    "    for y in range(height):\n",
    "        val = (img[x][y])\n",
    "        ax.annotate(str(val), xy=(y,x),\n",
    "                    horizontalalignment='center',\n",
    "                    verticalalignment='center',\n",
    "                    # if a pixel is black set the color of annotation as white\n",
    "                    color='white' if img[x][y]<thresh else 'black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "31e7d6612d8a9e594f01352292583c5e",
     "grade": false,
     "grade_id": "cell-dcd33ef8c7809e7f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Before we proceed to build and train the model, we will preprocess the data. First, let's choose only a subset for training data in order to reduce training time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f2f8a4ae247fce161717faa7f00021c6",
     "grade": false,
     "grade_id": "cell-7958e182f252f7fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# select subset of trainval_images and trainval_labels\n",
    "X_trainval = trainval_images[:16000]\n",
    "y_trainval = trainval_labels[:16000]\n",
    "\n",
    "# select whole test set\n",
    "X_test = test_images\n",
    "y_test = test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "08f14ebae964a7a72ee81a4250c2cd2f",
     "grade": false,
     "grade_id": "cell-ce616e8b2149dde6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Next, we need to reshape feature matrices `X_trainval` and `X_test` into the shape our model expects.\\\n",
    "The first layer of our ANN is the input layer with a dimensionality of 784 (28x28 pixel values/features per data point). Thus, we need to reshape the feature matrices `X_trainval` and `X_test` from  (-1, 28, 28) to (-1, 784):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f61b3edeb6e7586e538d503b70dfed5",
     "grade": false,
     "grade_id": "cell-4ab28310597908ca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_trainval = X_trainval.reshape(-1, 28 * 28)\n",
    "X_test = test_images.reshape(-1, 28 * 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b0f8448cb8a66c1d6307aa8fb461bb82",
     "grade": false,
     "grade_id": "cell-3406290cabb4620c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We refer to this stacking of a two-dimensional array of pixel grayscale values into a one-dimensional vector $\\mathbf{x}$ as **flattening**. \n",
    "\n",
    "When training the ANN it is a good practice to normalize the input values so that they are between 0 and 1, in our case, the pixel values.\n",
    "\n",
    "Let's transform feature values of type uint8 in a range [0, 255] to feature values of type float in the range [0, 1]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e8d200cf6e162e986e386865d4a510cc",
     "grade": false,
     "grade_id": "cell-a0445926e2a032be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Normalize data to have feature values between 0 and 1\n",
    "X_trainval = X_trainval/ 255.0\n",
    "X_test = X_test/ 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a2a17002ddc9d648ea3c7f438725188d",
     "grade": false,
     "grade_id": "cell-5354a6c46609a814",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 2. Define Hypothesis Space - ANN Structure\n",
    "\n",
    "To classify the shop items into 10 categories we learn a predictor map represented by an ANN. The ANN reads in the features $x_{i}$ of a shop item, which are the grayscale values of the item image. The output of the ANN are probabilities, or degrees of belonging, for each of the ten different categories. \n",
    "\n",
    "In particular, we will use an ANN with an arbitrary architecture:\n",
    "\n",
    "- input layer \n",
    "- one dense layer with 128 units and ReLU activation function\n",
    "- output layer with 10 units and softmax activation function\n",
    "\n",
    "The input layer consists of the individual features and is the entry point to the ANN. The input layer is connected to a dense layer with 128 neurons with the ReLU activation function. This hidden layer is then followed by the final output layer with ten neurons and a softmax activation function. \n",
    "\n",
    "The output layer with 10 neurons (corresponding to 10 classes) returns probabilities of belonging to a certain class. Note, that output values of all 10 neurons always sum to one. This is the result of using the softmax activation function in the last layer. For example, if the output of a first neuron (out of 10) is close to 1, we are confident in assigning the data point to class $1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4c4b2f77ecc2082b1c171d0eaaedec2d",
     "grade": false,
     "grade_id": "cell-0c2bdf5dee202e2e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "There are three main ways to build ANN in tensorflow backend Keras. The easiest one is to use [keras.Sequential() class](https://keras.io/api/models/sequential/). This class is used when you just need to stack layers sequentially. In contrast, [Functional API](https://keras.io/guides/functional_api/) is more flexible and allows building models with non-sequentional structure, multiple inputs and outputs. Finally, for more advanced users Keras provides [Dynamic models with Subclussing API](https://www.tensorflow.org/guide/keras/custom_layers_and_models).\n",
    "\n",
    "Here we will use a simple Sequential model. Below you will see 3 variations of how to build a Sequential model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0638bf10ee9caadb433be22fff774f44",
     "grade": false,
     "grade_id": "cell-b1b01371520684fb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# method 1\n",
    "# create an object - 'model' that represents an ANN\n",
    "model = keras.Sequential()      \n",
    "# add layers by using `model.add()` method\n",
    "model.add(layers.InputLayer(input_shape=(784,)))\n",
    "model.add(layers.Dense(units=128, activation='relu'))\n",
    "model.add(layers.Dense(units=10, activation='softmax'))\n",
    "\n",
    "# method 2\n",
    "# make a list of layers and pass to `keras.Sequential()`\n",
    "model = keras.Sequential([\n",
    "    layers.InputLayer(input_shape=(784,)),\n",
    "    layers.Dense(units=128, activation='relu'),\n",
    "    layers.Dense(units=10, activation='softmax')\n",
    "])\n",
    "\n",
    "# method 3\n",
    "# skip input layer and indicate input shape in the first hidden layer instead\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(units=128, activation='relu', input_shape=(784,)),\n",
    "    layers.Dense(units=10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "11a7a8e845e3fdfa804649a09dea1a89",
     "grade": false,
     "grade_id": "cell-35c496f237330559",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The last variant seems to be cleaner, so let's use it to define the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "34fa333c662122bac43db0af13d547a6",
     "grade": false,
     "grade_id": "cell-e12b5af1bd8c1b79",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# define model architecture\n",
    "\n",
    "model = keras.Sequential([\n",
    "    # hidden layer\n",
    "    layers.Dense(128, activation='relu',input_shape=(784,)),\n",
    "    # output layer\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "130507d4b67b86d13fabb1afcee5df5c",
     "grade": false,
     "grade_id": "cell-3804e92a63cd4916",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can call the `summary()` method on the model to see information about it. This method provides the following textual information:\n",
    "- The number of parameters (weights) in each layer\n",
    "- The total number of parameters (weights) in the model\n",
    "- The layers and their order in the model\n",
    "- The output shape of each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0cd175fd1b7356c8d2dcbbbf717a8d2b",
     "grade": false,
     "grade_id": "cell-390f260dcfce6741",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "37d1556cb7881f600718a69f4a3e2419",
     "grade": false,
     "grade_id": "cell-d2398862c53def64",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The above summary indicates that the ANN has a total of $d=101,770$ parameters (weights and biases) that can be adjusted (trained) based on labeled training data. You can also compute the number of parameters manually: \n",
    "- number of parameters in hidden layer 784*128+128 = 100480\n",
    "- number of parameters in output layer 128*10+10 = 1290\n",
    "\n",
    "or generally, the number of parameters in a layer equals to (n.o. neurons in previous layer)*(n.o. neurons in current layer) + (n.o. neurons in current layer). The last term corresponds to the biases of each neuron.\n",
    "\n",
    "The basic theory of ML suggests that we would need at least the same number of training examples to reliably train the ANN weights. A (crude!) rule of thumb is to have ten times more training data points than learnable weights. This would amount to a training set of around one million data points for which we would need to know the correct labels. Obviously, we have a much smaller dataset, but let's see how our model performs in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "344b6947f023c31ac73f702fc07b7a02",
     "grade": false,
     "grade_id": "cell-99e11c02493ca6b9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The textual model summary above is useful for simple models. However, this textual summary can be difficult to interpret for ANN structures with multiple inputs or outputs. To this end, `tf.keras` provides a function called `plot_model()` to create a graphical summary of the ANN that might be easier to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "21f163bceee34a2777dff3782786dc64",
     "grade": false,
     "grade_id": "cell-b8e2332611383fd2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "keras.utils.plot_model(\n",
    "    model,\n",
    "    show_shapes=True, \n",
    "    show_layer_names=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7d1b0703c79251f77baddd63f7acfc5a",
     "grade": false,
     "grade_id": "cell-83a7530c4b943ee7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 3 - Choose Loss Function and Optimizer\n",
    "\n",
    "Before we can start the training of the ANN, i.e., the tuning of the ANN weights, we need to define:\n",
    "\n",
    "- the specific algorithm, optimizer, (usually variant of gradient descent) for tuning the weights of the ANN\n",
    "- the loss function used to measure the quality of particular choice for the weights\n",
    "- the metric to assess the performance of the final choice for the weights. \n",
    "\n",
    "The code snippet below illustrates how to choose the categorical cross-entropy as loss function, the accuracy (fraction of correctly classified data points) as the metric, and the \"RMSprop\" variant of GD. All these parameters are specified using the Keras function `compile()`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e83a7e338c3937a08b956fde2ae5b858",
     "grade": false,
     "grade_id": "cell-a07a38101726a038",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"info\">\n",
    "    <div  class=\"info-title\"><i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>&nbsp; Info</div><br>\n",
    "    <div class=\"wrap-up-content\">\n",
    "In Keras we use <code>categorical_crossentropy</code> loss function for multiclass classification & when labels are provided in one-hot representation. We use <code>sparse_categorical_crossentropy</code> when you want to provide labels as integers. What are the differences? In principle none, as they both compute categorical cross-entropy, but read more <a href=\"https://stackoverflow.com/questions/58565394/what-is-the-difference-between-sparse-categorical-crossentropy-and-categorical-c\">here</a>. <br>\n",
    "<br>\n",
    "For classification problems we will use accuracy metrics <code>metrics='sparse_categorical_accuracy'</code>, but <a href=\"https://keras.io/api/metrics/\">other metrics</a> are also provided by Keras.    \n",
    "</div>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d7c7c6ec2088bd963ebf5538f05ac4b",
     "grade": false,
     "grade_id": "cell-94a054d195cb0d18",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='RMSprop',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics='sparse_categorical_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9a9edc365ef13e44891329d0463e6ea6",
     "grade": false,
     "grade_id": "cell-b6cb25782c304f09",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"info\">\n",
    "    <div  class=\"info-title\"><i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>&nbsp; Info</div><br>\n",
    "    <div class=\"wrap-up-content\">\n",
    "    With statement:    \n",
    "    \n",
    "```python\n",
    ">>> model.compile(optimizer='RMSprop', ... )\n",
    "```\n",
    "we choose the optimizer RMSprop with <a href=\"https://keras.io/api/optimizers/rmsprop/\">default parameters</a>. \n",
    "To print out the default learning rate use:\n",
    "    \n",
    "```python\n",
    ">>> model.optimizer.learning_rate\n",
    "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
    "```\n",
    "   \n",
    "</div>   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ca272f8609dabc0993c83466370869ce",
     "grade": false,
     "grade_id": "cell-69b24529c208bdb0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 4. Training - Adjusting ANN Weights\n",
    "\n",
    "We are now ready to train our network, which is done via a call to the `.fit()` method. Any Keras object that represents an ANN also provides a `.fit()` method which optimizes the ANN weights. \n",
    "\n",
    "Before we run following line of code, let's understand parameters in `.fit()` function: validation_split, batch_size and epochs. \n",
    "\n",
    "- **validation_spit:** validation_split is used to separate a portion of our training data into a validation dataset and evaluate the performance of our model on that validation dataset for each epoch. We can specify the percentage we want to allocate to the validation set by setting this validation_split argument on the fit() function to a percentage of the size of our training dataset.\\\n",
    "     So, why do we need a validation set anyway? A validation set is used to estimate the performance of a model for a given \n",
    "     training run (epoch). Meaning we will use the training set to train the model and use the validation set (data that's not \n",
    "     seen by the model during training) to predict while the model is being trained.  This will help us to get insight into \n",
    "     whether our model is overfitting or not. We can find out if the model is overfitting or not by comparing the accuracy and \n",
    "     loss from our training samples to the validation accuracy and validation loss from our validation samples. Further, we can \n",
    "     also use these accuracy measures from these two different sets to tune the hyper-parameters (like learning rate, number of \n",
    "     epochs the model should be trained) of the model.\n",
    "\n",
    "\n",
    "- **batch_size**: Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of tensorflow datasets, generators, or keras.utils. sequence instances (since they are already generating batches of a certain size).\n",
    "\n",
    "\n",
    "- **epochs**: This parameter is the number of times we want the optimization algorithm to use the entire dataset for tuning the weights. In general, the training loss decreases with increasing the number of epochs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d2b1489f7308d7da45601c725274252c",
     "grade": false,
     "grade_id": "cell-949dedcc9a955a6d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "# track execution time\n",
    "\n",
    "if training==True:\n",
    "    history = model.fit(X_trainval, y_trainval, validation_split=0.2, batch_size=32, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c70f4c82affa80fc12a9935ac4172eb9",
     "grade": false,
     "grade_id": "cell-92e94a4c41927d75",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Read the Training Log !\n",
    " \n",
    "\n",
    "`model.fit()` method has a parameter `verbose`, which is set to `verbose=1` [by default](https://keras.io/api/models/model_training_apis/#fit-method). This parameter controls information printed out during training. In the beginning, you see information about epoch number and progress bar is printed:\n",
    "\n",
    "`Epoch 1/20`\\\n",
    "`[==>...........................]`\n",
    "\n",
    "In addition, the total elapsed time and the time per sample is also printed for each epoch:\n",
    "\n",
    "`Epoch 1/20`\\\n",
    "`400/400 [==============================] - 1s 3ms/step`\n",
    "\n",
    "400 is a number of batches per epoch: 16000*0.8/32\n",
    "\n",
    "And most importantly, you can track loss values and values of metrics (e.g. accuracy):\n",
    "\n",
    "`loss: 0.6978 - accuracy: 0.7581 - val_loss: 0.5176 - val_accuracy: 0.8094`\n",
    "\n",
    "Note, that training loss and training accuracy are running averages - their values are updated after processing each batch. Loss and accuracy on the validation set are evaluated only at the end of an epoch. \n",
    "\n",
    "If you will set `verbose=2`, the progress bar and time elapsed will not be printed out and `verbose=0` is a silent mode. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "79d099964f133d5ad275ceab519e9556",
     "grade": false,
     "grade_id": "cell-35c3f4f5be3e9d45",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Plot Training History \n",
    "\n",
    "A `.fit()` method not only determines a good choice for the weights but also returns a history object. The history object contains different metrics for each epoch of the training (optimization) process. \n",
    "\n",
    "The metrics recorded during the training process are stored as a dictionary in the 'history.history' attribute of the 'history' object. To get a better understanding of how well the training process went, it is useful to plot the metrics as a function of epoch number. In particular, these plots allow analyzing:\n",
    "\n",
    "- how fast the optimization algorithm improves the loss function by adjusting the weights\n",
    "- whether there is any significant progress made anymore\n",
    "- whether the resulting predictor is overfitting (validation loss much higher than training loss) \n",
    "\n",
    "Note! While training loss/accuracy is computed as a running average, which is updated after computing loss for the current batch, validation loss/accuracy is computed at the end of the epoch. This means that validation loss/accuracy values sometimes will look better than for the training set, but only because they were computed after multiple parameters' updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7c5f63ad2e9e7f4b8ea7a5f0674913af",
     "grade": false,
     "grade_id": "cell-0a27dfab12efc958",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd    # library for handling tabular data\n",
    "\n",
    "# plot training log\n",
    "if training==True:\n",
    "    pd.DataFrame(history.history).plot(figsize=(7,4))\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('epoch', fontsize=14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c88f8a960906aac629f6d0e3eac451ac",
     "grade": false,
     "grade_id": "cell-b0482760ce42b639",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can see from the plot that the model is overfitting: training loss is much lower than validation loss, while training accuracy is much higher than accuracy on the validation dataset (~0.92 vs ~0.86).\n",
    "\n",
    "### Evaluation on Test Set\n",
    "\n",
    "After completing the training of ANN, we should evaluate the performance of the ANN with the final weights. For this, we will use a set of labeled data points that is different from the training and the validation sets. Indeed, we use the training set to train (optimize/tune/adjust) the ANN weights and we also use the validation set to choose hyperparameters, e.g. decide when to stop training (see [EarlyStopping](https://keras.io/api/callbacks/early_stopping/) for more info). Thus, both, the training set and validation set are used to find the final ANN weights. Therefore we need a test set that is different from the training and the validation sets. \n",
    "\n",
    "Let's evaluate the accuracy of our model on the test set with the `.evaluate()` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8de68709d370f9a1015b4d0a7a7032d5",
     "grade": false,
     "grade_id": "cell-059382990d7f5bea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "print('Accuracy on test dataset:', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e213670bc730417a9b307c2358222d2e",
     "grade": false,
     "grade_id": "cell-38b003fdcd0ba70e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can see that the accuracy on the test dataset is similar to the accuracy on the validation dataset (~0.86)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "154728d73b797c0e5fe6bbbb4f68b9e0",
     "grade": false,
     "grade_id": "cell-17855ef834b86824",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='2.4'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student task.</b> ANN for regression problem.</h3>\n",
    "\n",
    "Points: 1.5p\n",
    "\n",
    "Now, as you are familiar with basic tools to work with a neural network, your task is to build and train ANN similar to described above, but for a regression problem. We will use [California housing dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html#sklearn-datasets-fetch-california-housing) from sklearn. \n",
    "    \n",
    "- ANN for regression should predict real numbers (not restricted to range 0-1 as in the case with predicting probabilities). \n",
    "- You can freely choose the architecture of ANN (number of layers, neurons) and training parameters (validation_split, batch_size, epochs). \n",
    "    \n",
    "- We advice to use ReLU activation function for neurons in hidden layer(s).\n",
    "    \n",
    "You should think about:\n",
    "    \n",
    "- What loss function is used for the prediction of continuous numeric value? \n",
    "- Do you need to apply the activation function to the output neuron? \n",
    "\n",
    "Refer to [table](#table) if in doubt. \n",
    "    \n",
    "</div>\n",
    "    \n",
    "<div class=\" alert alert-danger\">\n",
    "    <h3 align='center'><b>Evaluate your model on a test set and achieve test loss (MSE) $\\leq$ 0.42.</b></h3>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<div class=\"info\">\n",
    "    <div  class=\"info-title\"><i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>&nbsp; Info</div><br>\n",
    "    <div class=\"wrap-up-content\">\n",
    "You do not need to specify metrics in <code>model.compile()</code>, if loss and metrics are the same. For example for model compiled as:\n",
    "    \n",
    "```python\n",
    ">>> model.compile(optimizer='RMSprop', loss='mean_squared_error')\n",
    "```\n",
    "\n",
    "both, the loss and the metrics, are mean squared error (MSE).\n",
    " \n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "59f20a169955deaadab17534aa59eb21",
     "grade": false,
     "grade_id": "cell-f7696f498a63f864",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='2.4'></a>\n",
    "<div class=\" alert alert-danger\">\n",
    "    <h3 align='center'><b> Save your model !</b></h3>\n",
    "    \n",
    "<p>Save your model for later grading:</p>\n",
    "    \n",
    "- Save whole model (configuration + weights) with the  `model_reg.save(\"model_reg.h5\")` function \n",
    "- Trained model can be loaded with the `tf.keras.models.load_model(\"model_reg.h5\")` function \n",
    "- More about saving model <a href=\"https://keras.io/getting_started/faq/#what-are-my-options-for-saving-models\">in Keras FAQ</a>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "04652653b2a3c15d3a54a05e8ef30196",
     "grade": false,
     "grade_id": "cell-0ba9dc27d095bcbb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# function to load dataset\n",
    "def load_dataset():\n",
    "    \n",
    "    X, y = fetch_california_housing(return_X_y=True)\n",
    "    X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    \n",
    "    # scale feature values\n",
    "    scaler = StandardScaler()\n",
    "    X_trainval = scaler.fit_transform(X_trainval)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    return X_trainval, y_trainval, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a18daf81f3e6d877e587780d195edef",
     "grade": false,
     "grade_id": "cell-1021c2b10c94b2a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "X_reg_trainval, y_reg_trainval, X_reg_test, y_reg_test = load_dataset()\n",
    "\n",
    "# shape of train and test image\n",
    "print(f'Number of training and validation examples {X_reg_trainval.shape}')\n",
    "print(f'Number of test examples {X_reg_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "306f39a6f493310d11ee5698a798afab",
     "grade": false,
     "grade_id": "cell-2d7667de4f904413",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Write your solution inside if-loop. With \n",
    "\n",
    "`if training==True: \n",
    "    ....`\\\n",
    "`else: \n",
    "    model_reg = tf.keras.models.load_model(\"model_reg.h5\")`\n",
    "\n",
    "teachers can change the flag to `training==False` and skip the training part during automated grading, loading the saved model instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fba0f84d86bbd03a4770b90309746b99",
     "grade": false,
     "grade_id": "cell-b3fcde0e46f18411",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if training==True:\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()\n",
    "    model_reg = keras.Sequential([\n",
    "    # hidden layer\n",
    "    layers.Dense(48, activation='relu',input_shape=(8,)),\n",
    "    #layers.Dense(32, activation='relu',input_shape=(8,)),\n",
    "    layers.Dense(16, activation='relu',input_shape=(8,)),\n",
    "    # output layer\n",
    "    layers.Dense(1)\n",
    "])\n",
    "    model_reg.compile(optimizer='RMSprop',\n",
    "              loss='mse')\n",
    "    history = model_reg.fit(X_reg_trainval, y_reg_trainval, validation_split=0.2, batch_size=32, epochs=20, verbose=1)\n",
    "    model_reg.save('model_reg.h5')\n",
    "    \n",
    "else: \n",
    "    model_reg = tf.keras.models.load_model(\"model_reg.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "568011268366fa15bee9fb0be80efcc4",
     "grade": false,
     "grade_id": "cell-9075dc89b40f1cec",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# plot training log\n",
    "if training==True:\n",
    "    pd.DataFrame(history.history).plot(figsize=(8,4))\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('epoch', fontsize=14)\n",
    "    plt.ylabel('Loss', fontsize=14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c03686fdaa48af8c23f3a8bc0d3bcbbc",
     "grade": false,
     "grade_id": "cell-2bb738bd3c1f6d3e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# loss on test set\n",
    "\n",
    "test_loss = model_reg.evaluate(X_reg_test,y_reg_test, batch_size=128, verbose=0)\n",
    "print('MSE loss on test dataset:', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "20c5638d0be9e228bd1bb327ffc46e53",
     "grade": false,
     "grade_id": "cell-33c2fd7acce963c6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "\n",
    "assert test_loss <= 0.42, \"MSE loss is too large!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7e158a68b8622a3f3338a9448eea4d1f",
     "grade": true,
     "grade_id": "cell-89669f51195bc55d",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# this cell is for tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2c7530186c81b9cedac28662d714e443",
     "grade": false,
     "grade_id": "cell-8b8d3fd86b533375",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-success\">\n",
    "    <h3><b>Try it out. </b>Designed features.</h3>\n",
    "\n",
    "Although the main idea of deep learning is to move away from hand-crafted features to a data-driven approach, sometimes \"creating\" and adding more features helps to train ANNs.\n",
    "    \n",
    "Go to TensorFlow playground https://playground.tensorflow.org/ . Use the following parameters:\n",
    "\n",
    "- dataset - Spiral\n",
    "- features ${X}_{1}$ and ${X}_{2}$\n",
    "- 2 hidden layers with 6 units each, coupled with ReLU\n",
    "- learning rate 0.01\n",
    "    \n",
    "Train for about ~500 epochs. Take a close look at the train and test loss values.\n",
    "    \n",
    "Now, add the rest of the features. Try training again and notice the difference.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e27b1c3da3f56febbd33aea44de69c21",
     "grade": false,
     "grade_id": "cell-49cae81f1eb5f6ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Model parameters and hyperparameters\n",
    "\n",
    "We call weights and biases **parameters** of a model. We can find optimal parameters by feeding data to a certain algorithm, e.g. gradient descent. In other words, parameters can be inferred from the data we have. In contrast, there are **hyperparameters** of a model, which cannot be inferred from data. Some examples of hyperparameters are:\n",
    "\n",
    "- number of layers in ANN\n",
    "- number of units (neurons) in a layer\n",
    "- activation function\n",
    "- learning rate\n",
    "- batch size\n",
    "- number of epochs\n",
    "- optimizer \n",
    "\n",
    "Read this [blog post](https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/) if still unsure about difference between parameters and hyperparameters.\n",
    "\n",
    "It is quite difficult to estimate in advance which hyperparameters to use and usually one has to try it out experimentally. Nevertheless, there is some progress in this topic and it is an active area of research. For example, autoML (automated machine learning) solutions are already offered by major tech companies. \n",
    "\n",
    "You can try out Python sklearn autoML solution:\n",
    "- https://automl.github.io/auto-sklearn/master/\n",
    "- https://machinelearningmastery.com/auto-sklearn-for-automated-machine-learning-in-python/\n",
    "\n",
    "In practice, ANN with one or two hidden layers and about a hundred neurons per layer is enough for many cases. In more difficult problems making a deeper network might be beneficial. One simple approach would be to build a model which will overfit data and then add regularization (we will discuss such methods in the later notebooks).\n",
    "\n",
    "Quick reminder: we say that model overfits data, when it performs very well on a training set, but much worse on a validation set. Adding regularization usually improves models' performance on a validation set. \n",
    "\n",
    "The number of neurons in input and output layers depends on input and output dimensions. Previously we build ANN for the image classification task with 784 neurons in the input layer (corresponds to input image of size 28x28 pixels) and 10  - in the output layer (corresponds to 10 classes). As for hidden layers, the number of neurons can be the same in each layer. Some architectures (e.g. autoencoders) have more neurons in the first and last layers and fewer neurons in the middle, thus creating a \"bottleneck\". Unless it is made on purpose, creating a bottleneck is not advised as it may lead to the loss of some information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ebcac03d380671eb24257c7c0bc22ffb",
     "grade": false,
     "grade_id": "cell-a8a41e74b1bec384",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='2.5'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student task.</b> Hyperparameter tuning - Number of hidden layers.</h3>\n",
    "\n",
    "Points: 2+2p\n",
    "    \n",
    "Consider two ANN structures that have the same total number of hidden units but a different number of layers. In this task you compare the performance of these two models on **Fashion-MNIST classification dataset**.\n",
    "    \n",
    "The first model should consist of:\n",
    "    \n",
    "- one hidden layer with 256 units and ReLU activation\n",
    "- one output layer with 10 units and softmax activation function\n",
    "    \n",
    "    \n",
    "The second model should consist of:\n",
    "    \n",
    "- 4 hidden layers with 64 units and ReLU activations\n",
    "- one output layer with 10 units and softmax activation function\n",
    "\n",
    "You can freely choose [Keras optimizer](https://keras.io/api/optimizers/) and training parameters (validation_split, batch_size, epochs).\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\" alert alert-danger\">\n",
    "    <h3 align='center'><b>The accuracy on test set should be $\\geq$ 0.83 for both models.</b></h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6f84b1f02aa0d601ac5e78f44c2d55ec",
     "grade": false,
     "grade_id": "cell-81544eb82bd0490c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='2.4'></a>\n",
    "<div class=\" alert alert-danger\">\n",
    "    <h3 align='center'><b> Save both models as 'model_256.h5' and 'model_4x64.h5' </b></h3>\n",
    "    \n",
    "You need to save your models for later grading!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e2cdd104afbd4350363d9fe0cd7e6fe4",
     "grade": false,
     "grade_id": "cell-cfef4f5ce06b7966",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#------------MODEL 1--------------------#\n",
    "\n",
    "# build a model with one hidden layer with 256 units and ReLU activation\n",
    "model_256 = keras.Sequential([\n",
    "    layers.Dense(units=256, activation='relu'),\n",
    "    layers.Dense(units=10, activation='softmax')\n",
    "])\n",
    "# compile a model\n",
    "model_256.compile(optimizer='RMSprop',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics='sparse_categorical_accuracy')\n",
    "if training==True:\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()\n",
    "    # train a model\n",
    "    history_256 = model_256.fit(X_trainval, y_trainval, validation_split=0.2, batch_size=32, epochs=20, verbose=0)\n",
    "    # save model\n",
    "    model_256.save('model_256.h5')\n",
    "    \n",
    "else: \n",
    "    model_256 = tf.keras.models.load_model(\"model_256.h5\")\n",
    "    \n",
    "# evaluate a model\n",
    "test_loss, test_accuracy = model_256.evaluate(X_test, y_test, verbose=0)\n",
    "print('Accuracy on test dataset:', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "62a2681057f8f5e0a8b20ad83dd13a7b",
     "grade": true,
     "grade_id": "cell-d4524542f58340ff",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Perform some sanity checks on the solution\n",
    "\n",
    "assert len(model_256.layers) == 2\n",
    "assert model_256.layers[0].units==256\n",
    "assert model_256.layers[1].units==10\n",
    "print(\"Sanity checks passed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "deda86ebdb9bd712580b61eb0ea8d881",
     "grade": false,
     "grade_id": "cell-028ab1f50eb98675",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#------------MODEL 2--------------------#\n",
    "\n",
    "# build a model with 4 hidden layers with 64 units and ReLU activations\n",
    "# model_4x64 = keras.Sequential([...])\n",
    "model_4x64 = keras.Sequential([\n",
    "    layers.Dense(units=64, activation='relu'),\n",
    "    layers.Dense(units=64, activation='relu'),\n",
    "    layers.Dense(units=64, activation='relu'),\n",
    "    layers.Dense(units=64, activation='relu'),\n",
    "    layers.Dense(units=10, activation='softmax')\n",
    "])\n",
    "\n",
    "# compile a model\n",
    "model_4x64.compile(optimizer='RMSprop',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics='sparse_categorical_accuracy')\n",
    "# model_4x64.compile(...)\n",
    "\n",
    "if training==True:\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()\n",
    "    # train a model\n",
    "    history_4x64 = model_4x64.fit(X_trainval, y_trainval, validation_split=0.2, batch_size=32, epochs=20, verbose=0)\n",
    "    # history_4x64 = model_4x64.fit(X_trainval, y_trainval, ...)\n",
    "    # save model\n",
    "    model_4x64.save('model_4x64.h5') \n",
    "else: \n",
    "    model_4x64 = keras.models.load_model(\"model_4x64.h5\")\n",
    "\n",
    "# evaluate a model\n",
    "test_loss, test_accuracy = model_4x64.evaluate(X_test, y_test, verbose=0)\n",
    "print('Accuracy on test dataset:', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4ce779026d9642165c6bed8a08c311f8",
     "grade": true,
     "grade_id": "cell-f3457d426cfe9b62",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Perform some sanity checks on the solution\n",
    "\n",
    "assert len(model_4x64.layers) == 5\n",
    "assert model_4x64.layers[0].units==64\n",
    "assert model_4x64.layers[1].units==64\n",
    "assert model_4x64.layers[2].units==64\n",
    "assert model_4x64.layers[3].units==64\n",
    "assert model_4x64.layers[4].units==10\n",
    "print(\"Sanity checks passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ac6b38c613abd5ebd1a306cf80347144",
     "grade": false,
     "grade_id": "cell-63466f95d55c971a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='2.5'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student task. </b> Hyperparameter tuning - Learning rate.</h3> \n",
    "    \n",
    "Points: 2.5p\n",
    "\n",
    "In this exercise we will use the following ANN for **Fashion-MNIST classification dataset**:\n",
    "    \n",
    "- one hidden layer with 128 units and ReLU activation\n",
    "- one output layer with 10 units and softmax activation function   \n",
    "    \n",
    "Use optimizer - SGD, loss - `sparse_categorical_crossentropy` and  metrics -  `sparse_categorical_accuracy`. \\\n",
    "Training parameters:\\\n",
    "`history = model.fit(X_trainval, y_trainval, validation_split=0.2, batch_size=32, epochs=20)`\n",
    "    \n",
    "\\\n",
    "Your task is to implement a function `lrate(lrates)`. The input paramter of this function `lrates` is a list of learning rate values. This function should, for each entry of that list, \n",
    "- create a new ANN Sequential model\n",
    "- compile the model. In this step, you need to pass the learning rate value to the optimizer, instead of using the default learning rate. \n",
    "- train the model\n",
    "- compute the accuracy obtained on the test set and add this value to the list `test_acc`\n",
    "\n",
    "After completing the above steps for each value in the list `lrates`, the function should return the list `test_acc`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6dcc94514e9e69da62ca348416e535cc",
     "grade": false,
     "grade_id": "cell-8ceb967ba67851d5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<details>\n",
    "    <summary><span class=\"summary-title\">Hints</span></summary>\n",
    "    <div class=\"summary-content\">\n",
    "    <ul>\n",
    "       <li>\n",
    "           use the <code>test_acc.append()</code> method to add current test accuracy to the end of the list\n",
    "        </li><br>\n",
    "        <li>\n",
    "            to set a learning rate, pass <code>optimizer=tf.keras.optimizers.SGD(learning_rate=...)</code> to\n",
    "            <code>model.compile()</code>, instead of <code>optimizer='SGD'</code>\n",
    "        </li><br>   \n",
    "        <li>\n",
    "            set <code>verbose=0</code> for <code>model.fit()</code> and <code>model.evaluate()</code> to avoid printing out training logs\n",
    "       </li>\n",
    "    </ul>\n",
    "    </div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9de16485cb6f9869122db1ef8aaaa554",
     "grade": false,
     "grade_id": "cell-71aad207b808ba64",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "lrates = [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()\n",
    "\n",
    "# define function; use for-loop to iterate list values\n",
    "\n",
    "# def lrate(lrates):\n",
    "#     for ...\n",
    "def lrate(lrates):\n",
    "    test_acc = []\n",
    "    for lr in lrates:\n",
    "        model= keras.Sequential([\n",
    "            layers.Dense(units=128, activation='relu'),\n",
    "            layers.Dense(units=10, activation='softmax')\n",
    "])\n",
    "        # compile a model\n",
    "        model.compile(keras.optimizers.SGD(learning_rate=lr),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics='sparse_categorical_accuracy')\n",
    "        # train a model\n",
    "        history = model.fit(X_trainval, y_trainval, validation_split=0.2, batch_size=32, epochs=20, verbose=0)\n",
    "        test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "        test_acc.append(test_accuracy)\n",
    "    return test_acc\n",
    "if training==True:\n",
    "    test_acc = lrate(lrates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "226e4d3240cd92ae516ec6355acfce4e",
     "grade": false,
     "grade_id": "cell-9b6c119d91e60d2f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# plot graph lrate vs test accuracy\n",
    "if training==True:\n",
    "    fig, ax = plt.subplots(1,1, sharey=True, figsize=(5,3))\n",
    "    \n",
    "    ax.plot(lrates, test_acc)\n",
    "    plt.xlabel(\"learning rate\", fontsize=14)\n",
    "    plt.ylabel(\"Test accuracy\", fontsize=14)\n",
    "    plt.xscale('log')   \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "de17bca596b0acb064c50d0a2eeb3eac",
     "grade": false,
     "grade_id": "cell-ab98b18702c8e1ef",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "\n",
    "What learning rate is optimal (one that gives highest test accuracy) ?\n",
    "\n",
    "<ol>\n",
    "  <li>0.0001</li>\n",
    "  <li>0.001</li>\n",
    "  <li>0.01</li>\n",
    "  <li>0.1</li>\n",
    "  <li>1</li>\n",
    "  <li>10</li>\n",
    "</ol> \n",
    "\n",
    "Set the variable `answer` to the index of the correct answer. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b9940e9760d0e464f86bfe9d64a2e5fb",
     "grade": false,
     "grade_id": "cell-0648679ac90adde5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### STUDENT TASK. Optimizer learning rate.###\n",
    "# remove the line raise NotImplementedError() before testing your solution and submitting code\n",
    "\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()\n",
    "\n",
    "answer = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ffe17bbd1505870d2106219e84d69a41",
     "grade": true,
     "grade_id": "cell-0c0ea767403d756a",
     "locked": true,
     "points": 2.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is for tests\n",
    "\n",
    "assert answer in [1,2,3,4,5,6], '\"answer\" Value should be an integer between 1 and 6.'\n",
    "print('Sanity check tests passed!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eca5d20466e32ec21519e04a83a05e51",
     "grade": false,
     "grade_id": "cell-9253b8b77ad758ed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "# check that all models from student tasks are saved\n",
    "\n",
    "import os.path\n",
    "\n",
    "# check that model from \"Student task. ANN for regression problem.\" is saved\n",
    "assert os.path.isfile('model_reg.h5')==True, \"save model in 'Student task. ANN for regression problem.'\"\n",
    "\n",
    "# check that models from \"Student task. Number of hidden layers.\" is saved\n",
    "assert os.path.isfile('model_256.h5')==True, \"save model_256 in 'Student task. Number of hidden layers.'\"\n",
    "assert os.path.isfile('model_4x64.h5')==True, \"save model_4x64 in 'Student task. Number of hidden layers.'\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e60d1f8c8d00006c47b8707528d3ccca",
     "grade": false,
     "grade_id": "cell-a7b85ff850cc59b4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Analyze learning curves.\n",
    "\n",
    "During training, we always evaluate loss (and accuracy) on two datasets - training and validation. Why do we need a validation dataset? The training loss (and accuracy) is a poor estimate of a model's performance because it is calculated on the same dataset that was used to train the model. The training dataset only represents a small sample of all available data points, this model will likely fit this subset very well, but fail to generalize to data points outside of the training set. This is especially a problem for big networks trained on small datasets. The validation set allows a more \"fair\" estimation of model performance, as it is not used for tuning the model's parameters.\n",
    "\n",
    "Although overfitting is a much more common problem for deep networks, let's create some examples of underfitting and overfitting. Underfitting may happen, for example, if the network is too simple for a given problem (too few layers/ units in layers), thus lacking the capacity to learn useful representation from data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3b6e427ef38bd7244e7fa68277b74249",
     "grade": false,
     "grade_id": "cell-aaac9a83ffac4d2c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Underfitting**. Here we will make a model with only one hidden layer with 2 units. This will create a \"bottleneck\" architecture. Such kind of model will be able to fit well neither training nor validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f69728ac8f2e3899bbcd7e3621220a3e",
     "grade": false,
     "grade_id": "cell-0241aee7e50b66aa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model_under = keras.Sequential([\n",
    "    layers.Dense(2, activation='relu',input_shape=(784,)),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0df80642b5aaf7796481f7c2e80531db",
     "grade": false,
     "grade_id": "cell-1e917606601f82cd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Optimal**. Here we will create a model with one hidden layer with 32 units. It is rather simple model, but sufficient in the case of FMNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "882010cf375160408c7d20872871bb81",
     "grade": false,
     "grade_id": "cell-7f0bc746c17dfe33",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model_opt = keras.Sequential([\n",
    "    layers.Dense(32, activation='relu',input_shape=(784,)),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4790861f1418f9c84a086dc72f7fa8bb",
     "grade": false,
     "grade_id": "cell-e9461648bd0bb147",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Overfitting**. Here we will create a model with several hidden layer and many units. This should be enough to overfit on training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "412e40c0796c596e607993788b35fe81",
     "grade": false,
     "grade_id": "cell-69ae55a50ffa788c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model_over = keras.Sequential([\n",
    "    layers.Dense(256, activation='relu',input_shape=(784,)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "71a9bfa8308ab6c41ffc99b1f963876e",
     "grade": false,
     "grade_id": "cell-42abcd5d721002ed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, let's compile and train all three models and store results in a list `history_log`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c4f172cdbe47eeb33ac80eb1555a41f4",
     "grade": false,
     "grade_id": "cell-5ed34566d84dc81f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "models = [model_under, model_opt, model_over]\n",
    "history_log = []\n",
    "\n",
    "if training==True:\n",
    "    for model in models:\n",
    "        model.compile(optimizer='RMSprop',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics='sparse_categorical_accuracy')\n",
    "\n",
    "        history = model.fit(X_trainval, y_trainval, validation_split=0.2, batch_size=32, epochs=20, verbose=0)\n",
    "        history_log.append(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8778cccecc4c2b68426fd4e3c9d3767b",
     "grade": false,
     "grade_id": "cell-3705cc7a5e196513",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Plot training and validation losses for all models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6a4e08f2ef7e42894f781867943b1608",
     "grade": false,
     "grade_id": "cell-7fe578f763b7ceac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if training==True:\n",
    "    fig, ax = plt.subplots(1,3, sharey=True, figsize=(12,4))\n",
    "    title = [\"underfit\", \"optimal\", \"overfit\"]\n",
    "\n",
    "    for i in range(len(history_log)):\n",
    "        ax[i].plot(history_log[i].history['loss'])\n",
    "        ax[i].plot(history_log[i].history['val_loss'])\n",
    "        ax[i].spines[\"top\"].set_visible(False)\n",
    "        ax[i].spines[\"right\"].set_visible(False)\n",
    "        ax[i].set_title(title[i], fontsize=18)\n",
    "\n",
    "    ax[0].set_xlabel('epoch', fontsize=14)\n",
    "    ax[0].set_ylabel('Loss', fontsize=14)\n",
    "\n",
    "    plt.legend(['train', 'val'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "813e846531695ccfce6dab3019af273e",
     "grade": false,
     "grade_id": "cell-243fcda34d2f002e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As you can see, when the model underfits, it performs poorly on training and validation sets. In contrast, when the model overfits, the training loss is much smaller than the validation loss. The ideal situation is when training loss is low and slightly better than validation loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b90a658ac86d579366e057b3a4084c7f",
     "grade": false,
     "grade_id": "cell-79dfbc58fdda2d4b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Quiz \n",
    "\n",
    "Points: 0.25p per Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "08ad41845d2dab3f2f5090e1d7000cbd",
     "grade": false,
     "grade_id": "cell-fdcd97124cfc9725",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "     <h3><b>Question 1.1.</b></h3>\n",
    "\n",
    "Which of the following statements is correct?\n",
    "\n",
    "1. An ANN is a network like representation of non-linear maps between features and predicted label value\n",
    "\n",
    "2. We typically adjust (learn) the ANN parameters (weights and bias terms) by random search \n",
    "\n",
    "3. ANNs can only be used to predict numeric label values (regression tasks)\n",
    "\n",
    "4. ANNs that use only linear activation functions can still result overall in a non-linear map \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9ceccf34aac382e436c1cf207d9bb71b",
     "grade": false,
     "grade_id": "cell-924d345e75dd426f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# remove the line raise NotImplementedError() before testing your solution and submitting code\n",
    "\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()\n",
    "\n",
    "answer_11  = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1f5f45ec1d8ab405d79af21e69debfa2",
     "grade": true,
     "grade_id": "cell-92814d083e097b9c",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is for tests\n",
    "assert answer_11 in [1,2,3,4], '\"answer\" Value should be an integer between 1 and 4.'\n",
    "print('Sanity check tests passed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7598aee87547ba7a978608fdf0c10dcc",
     "grade": false,
     "grade_id": "cell-94244c58c0a82b55",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "     <h3><b>Question 1.2.</b></h3>\n",
    "\n",
    "Choose the correct statement:\n",
    "\n",
    "1. Every ANN with less than three hidden layers always represents a linear map between input features and output (predicted label)\n",
    "\n",
    "2. To obtain non-linear maps between input and output, ANNs must use neurons with non-linear activation functions\n",
    "\n",
    "3. ANNs with non-linear activation functions can only be used to predict numeric labels (regression tasks)\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "69ff88c99f4205a90f80e8fdab28d991",
     "grade": false,
     "grade_id": "cell-c42fa331be1ebf41",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# remove the line raise NotImplementedError() before testing your solution and submitting code\n",
    "\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()\n",
    "\n",
    "answer_12  = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b6e3cdd78eb7eb897ffa76223e6de0a5",
     "grade": true,
     "grade_id": "cell-73374c0f5fc67cc9",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is for tests\n",
    "assert answer_12 in [1,2,3], '\"answer\" Value should be an integer between 1 and 3.'\n",
    "print('Sanity check tests passed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9324f5b3fd6deb0052a2c64ade62da3b",
     "grade": false,
     "grade_id": "cell-c273f8fef6d540c2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "     <h3><b>Question 1.3.</b></h3>\n",
    "\n",
    "Choose the correct statement:\n",
    "\n",
    "\n",
    "1. The softmax activation function is typically used in the output layer of an ANN applied to a multi-class classification task\n",
    "\n",
    "2. The softmax activation function is a popular choice for the output layer of an ANN applied to a regression task (numeric label values)\n",
    "\n",
    "3. The ReLU activation function is often used in the output layer of an ANN applied to a binary classification task\n",
    "\n",
    "4. The ReLU activation function is often used in the output layer of an ANN applied to a multi-class classification task\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a3f24b19bd14c3a9b09c3c8f78b6148b",
     "grade": false,
     "grade_id": "cell-e177f033d030abd9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# remove the line raise NotImplementedError() before testing your solution and submitting code\n",
    "\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()\n",
    "\n",
    "answer_13  = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "25d5a58d28f2923bb19f7d37474ff7f4",
     "grade": true,
     "grade_id": "cell-fa9a9c86d07b02f4",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is for tests\n",
    "assert answer_13 in [1,2,3,4], '\"answer\" Value should be an integer between 1 and 4.'\n",
    "print('Sanity check tests passed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fda77e59cf1306a19417ce033702e587",
     "grade": false,
     "grade_id": "cell-637898b40043aa4d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "     <h3><b>Question 1.4.</b></h3>\n",
    "\n",
    "Consider the following model:\n",
    "    \n",
    "\n",
    "```python     \n",
    "model = keras.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=(784,)),  # hidden layer 1\n",
    "        layers.Dense(128, activation='softmax')                    # hidden layer 2\n",
    "    ])\n",
    "```\n",
    "\n",
    "\n",
    "What is the input shape of the hidden layer 2?\n",
    "         \n",
    "1. input_shape = (784,)\n",
    "2. input_shape = (256,)\n",
    "3. input_shape = (128,)\n",
    "4. input_shape = (1,)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b59f18f51563564822e87b1329a08147",
     "grade": false,
     "grade_id": "cell-3ba1d3fe338f9e56",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# remove the line raise NotImplementedError() before testing your solution and submitting code\n",
    "\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()\n",
    "\n",
    "answer_14  = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ec6e3c5d79d49b6090e863a902d2d455",
     "grade": true,
     "grade_id": "cell-486dc28bdfbe7584",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is for tests\n",
    "assert answer_14 in [1,2,3,4], '\"answer\" Value should be an integer between 1 and 4.'\n",
    "print('Sanity check tests passed!')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "351.4375px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
